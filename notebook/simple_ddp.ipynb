{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DistributedSampler\n",
    "\n",
    "# Common configuration values for distributed training.\n",
    "nnode = 2 # Number of nodes(mahcines)\n",
    "#gpu_per_node = 4 # Number of GPUs(Processes) per node.\n",
    "gpu_per_node = [2, 4] # Or you can make a list for number of GPUs in case that each machine has different num of them.\n",
    "backend = 'nccl' # Backend. Read pytorch.DDP document for more details.\n",
    "init_method = 'env://' # Current Value is default. Read pytorch.DDP document for more details.\n",
    "master_addr = '127.0.0.1' # Master address. Replace with the IP address of Node 0.\n",
    "master_port = '12345' # Master Port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual node configuration. You have to edit this part for each node(machine)\n",
    "node = 0 # Node ID. 0 is the Master node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "\n",
    "# Define your model, dataset, and some hyperparameters here. Edit below lines.\n",
    "# You can define these elsewhere, but just make sure they are defined before DistributedSampler and DDP comes out.\n",
    "nn_model = torch.nn.Module() # Replace with your own model to use.\n",
    "dataset = Dataset() # Replace with your own dataset.\n",
    "\n",
    "batch_size = 100 # Replace with your own batch_size# Common configuration values for distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    #Write your own code here\n",
    "    pass\n",
    "\n",
    "def infer():\n",
    "    #Write your own code here\n",
    "    pass\n",
    "\n",
    "def dist_main(local_rank:int, world_size:int, _rank0:int)->None:\n",
    "    r'''Entry point for each process.\n",
    "    The first argument is always local rank which is provided by torch.mp.\n",
    "    '''\n",
    "    global_rank = local_rank + _rank0\n",
    "\n",
    "    os.environ['MASTER_ADDR'] = master_addr\n",
    "    os.environ['MASTER_PORT'] = master_port\n",
    "\n",
    "    dist.init_process_group(backend, init_method, world_size=world_size, rank=global_rank)\n",
    "\n",
    "    sampler = DistributedSampler(dataset, world_size, global_rank)\n",
    "    # Replace the arguments as you wish except the sampler unless you need a customized sampler.\n",
    "    dataloader = DataLoader(dataset, batch_size, sampler=sampler, num_workers=4, pin_memory=True)\n",
    "    ddp_model = DDP(nn_model.to(local_rank), device_ids=[local_rank])\n",
    "\n",
    "    # Do Something Here\n",
    "    # train()...\n",
    "    # infer()...\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if isinstance(gpu_per_node, int):\n",
    "        world_size = nnode * gpu_per_node\n",
    "        local_size = gpu_per_node\n",
    "        _rank0 = node * gpu_per_node\n",
    "    elif isinstance(gpu_per_node, list):\n",
    "        world_size = sum(gpu_per_node)\n",
    "        local_size = gpu_per_node[node]\n",
    "        _rank0 = sum(gpu_per_node[0:node])\n",
    "    mp.spawn(dist_main, (world_size, _rank0), local_size, join=True, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
